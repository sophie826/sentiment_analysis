{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPveKIjuCWeP86D+iVOnXrV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophie826/sentiment_analysis/blob/main/SentimentAnalysis_MethodComparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob vaderSentiment scikit-learn transformers torch"
      ],
      "metadata": {
        "id": "Qj3SdiLukEO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "id": "vShECN0OkXUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np                        # for splitting text into sentences\n",
        "import matplotlib.pyplot as plt  # for drawing charts\n",
        "from textblob import TextBlob    # Method 1\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # Method 2\n",
        "from sklearn.feature_extraction.text import CountVectorizer            # Method 3\n",
        "from sklearn.naive_bayes import MultinomialNB                          # Method 3\n",
        "from transformers import pipeline                                      # Method 4"
      ],
      "metadata": {
        "id": "zaP3g1fmkyBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 1: LOAD TEXT FILES**"
      ],
      "metadata": {
        "id": "qOwIoe_flaQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"trump.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    trump_text = f.read()\n",
        "print(f\"   Total characters in file: {len(trump_text)}\")"
      ],
      "metadata": {
        "id": "VW91S0Snl0jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2: SPLIT SENTENCES**"
      ],
      "metadata": {
        "id": "EVExQPDjnWfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_sentences = re.split(r'(?<=[.!?])\\s+', trump_text.strip())\n",
        "\n",
        "# Only keep sentences that are long enough to be meaningful, for example, more than 20 words (cutomizable)\n",
        "sentences = [s.strip() for s in raw_sentences if len(s.strip()) > 20]\n",
        "\n",
        "print(f\"âœ… Found {len(sentences)} sentences\")\n",
        "print()\n",
        "print(\"--- First 3 sentences as a preview ---\")\n",
        "for i, s in enumerate(sentences[:3]):\n",
        "    print(f\"  [{i+1}] {s[:80]}...\")"
      ],
      "metadata": {
        "id": "TEndO1ojnbYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3: TRY DIFFERENT METHODS TO RUN SENTIMENT ANALYSIS**"
      ],
      "metadata": {
        "id": "i077MKF1qX0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METHOD 1: HYBRID APPROACH - TEXTBOLB**"
      ],
      "metadata": {
        "id": "2Y6cINBjoDIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb_scores = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    blob = TextBlob(sentence)\n",
        "    tb_scores.append(blob.sentiment.polarity)\n",
        "\n",
        "# Calculate the average polarity across all sentences\n",
        "tb_average = sum(tb_scores) / len(tb_scores)\n",
        "\n",
        "# Count how many sentences are positive, neutral, and negative\n",
        "tb_positive = sum(1 for s in tb_scores if s > 0.05)\n",
        "tb_neutral  = sum(1 for s in tb_scores if -0.05 <= s <= 0.05)\n",
        "tb_negative = sum(1 for s in tb_scores if s < -0.05)\n",
        "\n",
        "print(f\"  Average polarity score : {tb_average:+.3f}\")\n",
        "\n",
        "if tb_average > 0.05:\n",
        "    print(\"  Overall sentiment      : ðŸ˜Š POSITIVE\")\n",
        "elif tb_average < -0.05:\n",
        "    print(\"  Overall sentiment      : ðŸ˜ž NEGATIVE\")\n",
        "else:\n",
        "    print(\"  Overall sentiment      : ðŸ˜ NEUTRAL\")\n",
        "\n",
        "print()\n",
        "print(f\"  Positive sentences : {tb_positive}  ({tb_positive/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Neutral  sentences : {tb_neutral}  ({tb_neutral/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Negative sentences : {tb_negative}  ({tb_negative/len(sentences)*100:.1f}%)\")\n",
        "\n",
        "# Show the most positive sentence TextBlob found\n",
        "best_tb_score  = max(tb_scores)\n",
        "best_tb_index  = tb_scores.index(best_tb_score)\n",
        "worst_tb_score = min(tb_scores)\n",
        "worst_tb_index = tb_scores.index(worst_tb_score)\n",
        "\n",
        "print()\n",
        "print(f\"  Most POSITIVE sentence  [{best_tb_score:+.2f}]:\")\n",
        "print(f\"    \\\"{sentences[best_tb_index]}...\\\"\")\n",
        "print(f\"  Most NEGATIVE sentence  [{worst_tb_score:+.2f}]:\")\n",
        "print(f\"    \\\"{sentences[worst_tb_index]}...\\\"\")\n",
        "print()"
      ],
      "metadata": {
        "id": "8w63OwXWoHVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METHOD 2 â€” CLASSIC MACHINE LEARNING - NAIVE BAYES**"
      ],
      "metadata": {
        "id": "M_gXStMWorIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0-Cz6IBdo1Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step A: Create labels using textblob average scores ---\n",
        "labels = []\n",
        "for score in tb_scores:\n",
        "    if score >= 0.05:\n",
        "        labels.append(\"POSITIVE\")\n",
        "    elif score <= -0.05:\n",
        "        labels.append(\"NEGATIVE\")\n",
        "    else:\n",
        "        labels.append(\"NEUTRAL\")\n",
        "\n",
        "print(f\"  Labels created from TEXTBLOB scores:\")\n",
        "print(f\"    POSITIVE : {labels.count('POSITIVE')}\")\n",
        "print(f\"    NEUTRAL  : {labels.count('NEUTRAL')}\")\n",
        "print(f\"    NEGATIVE : {labels.count('NEGATIVE')}\")\n",
        "print()\n",
        "\n",
        "# --- Step B: Convert sentences into numbers ---\n",
        "# CountVectorizer counts how many times each word appears\n",
        "# For example: \"great economy great jobs\" â†’  {great: 2, economy: 1, jobs: 1}\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")  # ignore \"the\", \"a\", etc.\n",
        "X = vectorizer.fit_transform(sentences)             # X is now a matrix of word counts\n",
        "\n",
        "print(f\"  Vocabulary size (number of unique words): {len(vectorizer.vocabulary_)}\")\n",
        "print(f\"  Matrix shape: {X.shape}  (sentences Ã— unique words)\")\n",
        "print()\n",
        "\n",
        "# --- Step C: Train the Naive Bayes model ---\n",
        "# MultinomialNB works well with word count features\n",
        "model = MultinomialNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "# --- Step D: Predict sentiment for each sentence ---\n",
        "predictions = model.predict(X)\n",
        "\n",
        "nb_positive = list(predictions).count(\"POSITIVE\")\n",
        "nb_neutral  = list(predictions).count(\"NEUTRAL\")\n",
        "nb_negative = list(predictions).count(\"NEGATIVE\")\n",
        "\n",
        "print(f\"  Training accuracy: {(predictions == labels).mean() * 100:.1f}%\")\n",
        "print()\n",
        "print(f\"  Positive sentences : {nb_positive}  ({nb_positive/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Neutral  sentences : {nb_neutral}  ({nb_neutral/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Negative sentences : {nb_negative}  ({nb_negative/len(sentences)*100:.1f}%)\")\n",
        "\n",
        "# --- Step E: Show the most \"positive\" words the model learned ---\n",
        "# We look at the class probabilities the model assigned to each word\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "class_index   = list(model.classes_).index(\"POSITIVE\")\n",
        "top_positive_indices = model.feature_log_prob_[class_index].argsort()[-10:]\n",
        "top_positive_words   = [feature_names[i] for i in top_positive_indices]\n",
        "\n",
        "print()\n",
        "print(f\"  Top words associated with POSITIVE sentiment:\")\n",
        "print(f\"    {', '.join(top_positive_words)}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "L33GbxNOo7Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**METHOD 3 â€” LLM - DistilBERT**"
      ],
      "metadata": {
        "id": "Wbgus22bp7ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "# a RoBERTa model trained on ~124 million tweets and fine-tuned\n",
        "# for 3-class sentiment: Positive / Neutral / Negative.\n",
        "\n",
        "# Load the 3-class sentiment pipeline\n",
        "bert_analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    truncation=True,   # cut sentences longer than 512 tokens\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "bert_labels     = []\n",
        "bert_confidence = []\n",
        "\n",
        "# Process sentences in small batches (faster than one-by-one)\n",
        "batch_size = 16\n",
        "for i in range(0, len(sentences), batch_size):\n",
        "    batch   = sentences[i : i + batch_size]\n",
        "    results = bert_analyzer(batch)\n",
        "    for result in results:\n",
        "        # The model returns \"positive\", \"neutral\", \"negative\" (lowercase)\n",
        "        # We convert to uppercase to match our other methods\n",
        "        bert_labels.append(result[\"label\"].upper())\n",
        "        bert_confidence.append(result[\"score\"])\n",
        "\n",
        "bert_positive = bert_labels.count(\"POSITIVE\")\n",
        "bert_neutral  = bert_labels.count(\"NEUTRAL\")\n",
        "bert_negative = bert_labels.count(\"NEGATIVE\")\n",
        "avg_conf      = sum(bert_confidence) / len(bert_confidence)\n",
        "\n",
        "print(f\"  Positive sentences : {bert_positive}  ({bert_positive/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Neutral  sentences : {bert_neutral}  ({bert_neutral/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Negative sentences : {bert_negative}  ({bert_negative/len(sentences)*100:.1f}%)\")\n",
        "print(f\"  Average confidence : {avg_conf:.3f}  (how sure the model is)\")\n",
        "\n",
        "# Show a couple of example predictions with confidence\n",
        "print()\n",
        "print(\"  Example predictions:\")\n",
        "for i in [0, 1, 2]:\n",
        "    s    = sentences[i]\n",
        "    lbl  = bert_labels[i]\n",
        "    conf = bert_confidence[i]\n",
        "    emoji = \"ðŸ˜Š\" if lbl == \"POSITIVE\" else \"ðŸ˜ž\" if lbl == \"NEGATIVE\" else \"ðŸ˜\"\n",
        "    print(f\"    {emoji} {lbl} ({conf:.2f}) â€” \\\"{s[:70]}...\\\"\")"
      ],
      "metadata": {
        "id": "kmE6bIcep-9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 4: COMPARE RESULTS**"
      ],
      "metadata": {
        "id": "ONdKCnmyqTVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = [\"TextBlob\", \"Naive Bayes\", \"RoBERTa\"]\n",
        "\n",
        "# Collect % for each category from each method\n",
        "positive_pcts = [\n",
        "    tb_positive    / len(sentences) * 100,\n",
        "    nb_positive    / len(sentences) * 100,\n",
        "    bert_positive  / len(sentences) * 100,\n",
        "]\n",
        "neutral_pcts = [\n",
        "    tb_neutral     / len(sentences) * 100,\n",
        "    nb_neutral     / len(sentences) * 100,\n",
        "    bert_neutral   / len(sentences) * 100,   # RoBERTa now has Neutral\n",
        "]\n",
        "negative_pcts = [\n",
        "    tb_negative    / len(sentences) * 100,\n",
        "    nb_negative    / len(sentences) * 100,\n",
        "    bert_negative  / len(sentences) * 100,\n",
        "]\n",
        "\n",
        "# Print a simple summary table\n",
        "print(f\"  {'Method':<14}  {'Positive':>9}  {'Neutral':>9}  {'Negative':>9}\")\n",
        "print(\"  \" + \"-\" * 47)\n",
        "for i, m in enumerate(methods):\n",
        "    print(f\"  {m:<14}  {positive_pcts[i]:>8.1f}%  \"\n",
        "          f\"{neutral_pcts[i]:>8.1f}%  {negative_pcts[i]:>8.1f}%\")\n",
        "print()\n",
        "\n",
        "# --- Build the chart ---\n",
        "fig, ax = plt.subplots(figsize=(13, 6))\n",
        "fig.patch.set_facecolor(\"#F0F4F8\")\n",
        "ax.set_facecolor(\"white\")\n",
        "\n",
        "x     = np.arange(len(methods))  # x positions for the 4 method groups\n",
        "width = 0.25                      # width of each bar\n",
        "\n",
        "# Three bars per group, shifted left / centre / right\n",
        "bars_pos = ax.bar(x - width, positive_pcts, width,\n",
        "                  label=\"Positive\", color=\"#059669\",\n",
        "                  edgecolor=\"white\", linewidth=1.0)\n",
        "bars_neu = ax.bar(x,          neutral_pcts,  width,\n",
        "                  label=\"Neutral\",  color=\"#94A3B8\",\n",
        "                  edgecolor=\"white\", linewidth=1.0)\n",
        "bars_neg = ax.bar(x + width, negative_pcts, width,\n",
        "                  label=\"Negative\", color=\"#E11D48\",\n",
        "                  edgecolor=\"white\", linewidth=1.0)\n",
        "\n",
        "# Add a % label above each bar (skip bars too short to read)\n",
        "def label_bars(bars):\n",
        "    for bar in bars:\n",
        "        h = bar.get_height()\n",
        "        if h > 1:\n",
        "            ax.text(bar.get_x() + bar.get_width() / 2, h + 0.8,\n",
        "                    f\"{h:.1f}%\", ha=\"center\", va=\"bottom\",\n",
        "                    fontsize=9, fontweight=\"bold\", color=\"#1A2B4A\")\n",
        "\n",
        "label_bars(bars_pos)\n",
        "label_bars(bars_neu)\n",
        "label_bars(bars_neg)\n",
        "\n",
        "# Style\n",
        "ax.set_title(\"Sentiment Distribution by Method â€” Trump Speech Corpus\",\n",
        "             fontsize=14, fontweight=\"bold\", color=\"#1A2B4A\", pad=15)\n",
        "ax.set_ylabel(\"% of Sentences\", fontsize=11, color=\"#1A2B4A\")\n",
        "ax.set_ylim(0, 105)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods, fontsize=12)\n",
        "ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
        "ax.grid(axis=\"y\", alpha=0.25)\n",
        "ax.legend(fontsize=11, loc=\"upper right\", framealpha=0.85, edgecolor=\"#DDE3ED\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "pARC_ydwq1Cs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}